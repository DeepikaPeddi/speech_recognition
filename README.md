# SPEECH RECOGNITION SYSTEM

*COMPANY* : CODTECH IT SOLUTIONS

*NAME* : DEEPIKA PEDDI

*INTERN ID* : CT06DZ1375

*DOMAIN* : ARTIFICIAL INTELLIGENCE

*DURATION* : 6 WEEKS

*MENTOR* : NEELA SANTOSH

## As part of my AI internship experience at CodTech IT Solutions, I created a Speech Recognition System that can translate audio to text. This was the second task in my internship, which involved creating useful AI-based products using machine learning and NLP principles. The primary goal of this project was to develop and deploy a simple but working speech-to-text (STT) system that takes in a specified audio input and generates its textual equivalent as output. This activity taught me how machines interpret human language and how audio signals can be transformed into understandable text using AI models and APIs. In order to execute this work, I employed Python and the SpeechRecognition library, which is possibly one of the easiest yet most capable tools available for audio-to-text conversion. The principal tool employed was the Google Web Speech API, which is called upon through the SpeechRecognition library. This API has the best transcription accuracy available for English audio and is commonly used in small to medium-sized level projects and prototypes. The system takes .wav audio files and breaks down the spoken words into readable text using this API. The development and run were both carried out completely on Google Colab, which was the editor platform for this project. Google Colab was used because it is cloud-based, does not need any local installation, and provides very easy file uploads (such as .wav files) through built-in functionality such as files.upload(). It also has real-time execution of Python code in Jupyter notebook format, and using that, I could test and enhance the speech recognition logic incrementally. I uploaded my code and ran each block individually in Colab to construct the workflow from importing packages to uploading an audio file and finally showing the transcribed result.As a demonstration example, I used a popular known audio file known as harvard.wav that has clearly articulated English sentences. This file is often employed for tutorials and demonstrations since it contains common sentences with a range of vocabulary and phonetic variation. Employing this audio assisted in testing the quality of transcription and ensured the model was effectively transcribing speech into text. The output was returned clean and legible, illustrating that the speech recognizer accurately read the input correctly.Practically, this Speech Recognition System has extensive real-world applications. It can be employed in voice-controlled applications, virtual assistants, automatic generation of subtitles, meeting or interview transcription, hands-free control systems, and even in accessibility tools for the disabled. In the healthcare, education, customer service, and media industries, speech-to-text systems have the potential to significantly increase productivity, documentation, and user interaction. Whereas this project works with offline audio files, future revisions could involve microphone-driven live transcription, multi-language capabilities, or seamless integration with cutting-edge models such as Wav2Vec2.0 or OpenAI's Whisper for stronger and more scalable implementations.

## Overall, this project familiarized me with the basics of speech processing and taught me how to implement AI tools in real-world voice interaction. I now understand the pipeline of converting audio signals into machine-readable formats and how to use APIs to achieve high-accuracy speech recognition. The final notebook (speech_to_text.ipynb) and the sample audio file (harvard.wav) are included in this repository. This task served as a strong foundation for future projects in AI audio analysis, and Iâ€™m excited to explore more advanced audio-based AI solutions going forward.

# output

<img width="1862" height="683" alt="Image" src="https://github.com/user-attachments/assets/f4808496-fc53-4a6c-af6a-35d71ea82c75" />
